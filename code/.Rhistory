# ---- final table builder (no 'state' column from raster) ----
finalize_table <- function(pairs_dt, totals_dt, from_map, to_map, to_names, target_label) {
if (!nrow(pairs_dt)) return(data.frame())
from_lu <- from_map |> rename(CZ20 = id, code_from = code)
to_lu   <- to_map   |> left_join(to_names, by = "id") |>
rename(!!target_label := id, !!paste0(target_label,"_name") := name, code_to = code)
res <- pairs_dt |>
left_join(totals_dt, by="code_from") |>
left_join(from_lu,   by="code_from") |>
left_join(to_lu,     by="code_to") |>
transmute(
CZ20,
!!target_label := .data[[target_label]],
!!paste0(target_label,"_name") := .data[[paste0(target_label,"_name")]],
pop_overlap = pop_overlap,
share_of_CZ20 = ifelse(pop_from > 0, pop_overlap / pop_from, 0),
afactor       = share_of_CZ20,
CZ20_total_pop = pop_from,
cells = cells,
grid_res_m = !!cell_m,
crs_epsg   = !!epsg,
method     = !!method_tag
) |>
as.data.frame()
# Add PUMA state_fips/state when the target is PUMA*
if (target_label %in% c("PUMA20","PUMA2010")) {
fips_to_usps <- c(
"01"="AL","02"="AK","04"="AZ","05"="AR","06"="CA","08"="CO","09"="CT","10"="DE","11"="DC",
"12"="FL","13"="GA","15"="HI","16"="ID","17"="IL","18"="IN","19"="IA","20"="KS","21"="KY",
"22"="LA","23"="ME","24"="MD","25"="MA","26"="MI","27"="MN","28"="MS","29"="MO","30"="MT",
"31"="NE","32"="NV","33"="NH","34"="NJ","35"="NM","36"="NY","37"="NC","38"="ND","39"="OH",
"40"="OK","41"="OR","42"="PA","44"="RI","45"="SC","46"="SD","47"="TN","48"="TX","49"="UT",
"50"="VT","51"="VA","53"="WA","54"="WV","55"="WI","56"="WY","60"="AS","66"="GU","69"="MP",
"72"="PR","78"="VI"
)
tid <- res[[target_label]]
state_fips <- stringr::str_sub(tid, 2, 3)
res$state_fips <- state_fips
res$state      <- unname(fips_to_usps[state_fips])
}
res
}
# ---- read polygons, add codes ----
cz20   <- read_std(file.path(dir_2020, "cz20.shp"),                 "CZ20")
cz90   <- read_std(file.path(dir_1990, "cz.shp"),                   "cz",      "cz_name")
puma20 <- read_std(file.path(dir_2020, "ipums_puma_2020.shp"),      "GISJOIN", "Name")
puma10 <- read_std(file.path(dir_2010, "ipums_puma_2010_tl20.shp"), "GISJOIN", "Name")
ers10  <- read_std(file.path(dir_cz10, "ERS10.rep.shp"),            "LM_Code")
ers00  <- read_std(file.path(dir_cz00, "ERS00.shp"),                "LM_Code")
cz20_map   <- make_code_map(cz20);   cz20   <- left_join(cz20,   cz20_map,   by="id")
cz90_map   <- make_code_map(cz90);   cz90   <- left_join(cz90,   cz90_map,   by="id")
puma20_map <- make_code_map(puma20); puma20 <- left_join(puma20, puma20_map, by="id")
puma10_map <- make_code_map(puma10); puma10 <- left_join(puma10, puma10_map, by="id")
ers10_map  <- make_code_map(ers10);  ers10  <- left_join(ers10,  ers10_map,  by="id")
ers00_map  <- make_code_map(ers00);  ers00  <- left_join(ers00,  ers00_map,  by="id")
nm_cz90 <- st_drop_geometry(cz90)   |> select(id, name)
nm_p20  <- st_drop_geometry(puma20) |> select(id, name)
nm_p10  <- st_drop_geometry(puma10) |> select(id, name)
nm_e10  <- st_drop_geometry(ers10)  |> select(id, name)
nm_e00  <- st_drop_geometry(ers00)  |> select(id, name)
cz20_for_rast   <- cz20   |> select(code, geometry)
cz90_for_rast   <- cz90   |> select(code, geometry)
puma20_for_rast <- puma20 |> select(code, geometry)
puma10_for_rast <- puma10 |> select(code, geometry)
ers10_for_rast  <- ers10  |> select(code, geometry)
ers00_for_rast  <- ers00  |> select(code, geometry)
# ---- NATIONAL run over ALL state TIFs ----
tifs <- list.files(grid_dir, pattern = "^[A-Z]{2}_grid100m_pop_2020_.*\\.tif$", full.names = TRUE)
stopifnot(length(tifs) > 0)
acc_totals <- data.table(code_from=integer(), pop_from=numeric())  # national CZ20 totals on grid
acc <- list(
CZ90     = data.table(),
PUMA20   = data.table(),
PUMA2010 = data.table(),
CZ2010   = data.table(),
CZ2000   = data.table()
)
for (ti in tifs) {
message("\n----- ", basename(ti), " -----")
r <- terra::rast(ti) |> ensure_crs(epsg)
bb <- bbox_sf(r)
# source = CZ20 inside raster bbox
from_s <- suppressWarnings(st_intersection(cz20_for_rast, bb))
if (nrow(from_s) == 0L) { gc(); next }
# totals for this raster
t_res <- pairs_for_target(r, from_s, from_s)$totals
if (nrow(t_res)) {
if (nrow(acc_totals)) {
acc_totals <- merge(acc_totals, t_res, by="code_from", all=TRUE, suffixes=c(".x",".y"))
acc_totals[, pop_from := rowSums(.SD, na.rm=TRUE), .SDcols=c("pop_from.x","pop_from.y")]
acc_totals[, c("pop_from.x","pop_from.y") := NULL]
} else acc_totals <- t_res
}
# accumulate pairs for each target
do_one <- function(target_for_rast, key) {
t_s <- suppressWarnings(st_intersection(target_for_rast, bb))
if (nrow(t_s)==0L) return(invisible(NULL))
res <- pairs_for_target(r, from_s, t_s)$pairs
if (nrow(res)) acc[[key]] <<- rbindlist(list(acc[[key]], res), use.names=TRUE, fill=TRUE)
invisible(NULL)
}
do_one(cz90_for_rast,   "CZ90")
do_one(puma20_for_rast, "PUMA20")
do_one(puma10_for_rast, "PUMA2010")
do_one(ers10_for_rast,  "CZ2010")
do_one(ers00_for_rast,  "CZ2000")
gc()
}
# collapse duplicates across rasters (national)
collapse_nat <- function(dt) {
if (!nrow(dt)) return(dt)
dt[, .(cells = sum(cells), pop_overlap = sum(pop_overlap)), by = .(code_from, code_to)]
}
acc <- lapply(acc, collapse_nat)
# ---- finalize & write (one CSV per geography) ----
final_write <- function(key, target_label, to_map, to_names, filename) {
if (!nrow(acc[[key]])) { message("No rows for ", key); return(invisible(NULL)) }
out <- finalize_table(
pairs_dt  = acc[[key]],
totals_dt = acc_totals,
from_map  = cz20_map,
to_map    = to_map,
to_names  = to_names,
target_label = target_label
)
# Optional QA: afactor sums for each CZ20
qa <- out |>
group_by(CZ20) |>
summarise(afactor_sum = sum(afactor, na.rm=TRUE), .groups="drop") |>
filter(abs(afactor_sum - 1) > 1e-6)
if (nrow(qa)) message("WARNING (", target_label, "): ", nrow(qa), " CZ20(s) not summing to 1.")
readr::write_csv(out, file.path(out_dir, filename))
message("Wrote: ", filename, " (", nrow(out), " rows)")
}
final_write("CZ90",     "CZ90",     cz90_map,   nm_cz90, "cz20_to_cz90_popgrid100m_area.csv")
final_write("PUMA20",   "PUMA20",   puma20_map, nm_p20,  "cz20_to_puma20_popgrid100m_area.csv")
final_write("PUMA2010", "PUMA2010", puma10_map, nm_p10,  "cz20_to_puma2010_popgrid100m_area.csv")
final_write("CZ2010",   "CZ2010",   ers10_map,  nm_e10,  "cz20_to_cz2010_popgrid100m_area.csv")
final_write("CZ2000",   "CZ2000",   ers00_map,  nm_e00,  "cz20_to_cz2000_popgrid100m_area.csv")
# (Optional) national CZ20 totals on the grid for QA
readr::write_csv(as.data.frame(acc_totals), file.path(out_dir, "cz20_grid_totals_NATIONAL.csv"))
message("\nDone. Crosswalks in: ", out_dir)
sums <- out |>
group_by(PUMA20) |>
summarise(afactor_sum = sum(afactor, na.rm=TRUE), .groups = "drop")
View(cz20)
gc()
suppressPackageStartupMessages({
library(dplyr); library(readr); library(stringr); library(tibble)
})
suppressPackageStartupMessages({
library(dplyr); library(readr); library(stringr); library(tibble)
})
# ---- paths ----
base_root <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk"
out_dir   <- file.path(base_root, "output", "crosswalks")
qa_dir    <- file.path(out_dir, "qa"); dir.create(qa_dir, showWarnings = FALSE, recursive = TRUE)
# ---- settings ----
tol <- 1e-6  # tolerance for |sum(afactor)-1|
digits_report <- 12  # round in report for readability
# ---- helpers ----
safe_read <- function(fp) {
tryCatch(readr::read_csv(fp, show_col_types = FALSE), error = function(e) NULL)
}
check_sums <- function(df, source_col, weight_col = "afactor", file_label = NA_character_) {
if (is.null(df)) return(tibble())
need <- c(source_col, weight_col)
if (!all(need %in% names(df))) {
warning("Missing required columns in ", file_label, ": ", paste(setdiff(need, names(df)), collapse=", "))
return(tibble())
}
df |>
transmute(src = .data[[source_col]], w = .data[[weight_col]]) |>
group_by(src) |>
summarise(
afactor_sum = sum(w, na.rm = TRUE),
n_parts     = n(),
n_na        = sum(is.na(w)),
min_part    = suppressWarnings(min(w, na.rm = TRUE)),
max_part    = suppressWarnings(max(w, na.rm = TRUE)),
.groups = "drop"
) |>
mutate(
abs_err   = abs(afactor_sum - 1),
ok        = abs_err <= tol
) |>
filter(!ok) |>
mutate(
afactor_sum = round(afactor_sum, digits_report),
abs_err     = round(abs_err, digits_report),
file        = file_label,
source_col  = source_col
) |>
select(file, source_col, src, afactor_sum, abs_err, n_parts, n_na, min_part, max_part)
}
write_report <- function(tbl, out_path) {
if (nrow(tbl)) {
readr::write_csv(tbl, out_path)
message("Wrote QA report: ", out_path, " (", nrow(tbl), " bad sources)")
} else {
message("All good: ", out_path, " (no bad sources)")
# still drop an empty file so the run is traceable
readr::write_csv(tibble(note="no issues found", generated=as.character(Sys.time())), out_path)
}
}
# ---- discover files ----
all_csvs <- list.files(out_dir, pattern = "\\.csv$", full.names = TRUE)
# PUMA20 -> CZ90 national
puma_file <- all_csvs[str_detect(basename(all_csvs), "^puma20_to_cz90_.*_NATIONAL\\.csv$")]
# CZ20 -> {CZ90,PUMA20,PUMA2010,CZ2010,CZ2000} nationals
cz20_files <- all_csvs[str_detect(basename(all_csvs), "^cz20_to_.*_NATIONAL\\.csv$")]
# ---- run checks ----
summary_rows <- list()
# 1) PUMA20->CZ90: group by PUMA20
if (length(puma_file)) {
for (fp in puma_file) {
df <- safe_read(fp)
bad <- check_sums(df, source_col = "PUMA20", weight_col = "afactor", file_label = basename(fp))
outp <- file.path(qa_dir, paste0(tools::file_path_sans_ext(basename(fp)), "_QA_bad.csv"))
write_report(bad, outp)
if (nrow(bad)) summary_rows[[length(summary_rows)+1]] <- bad
}
} else {
message("No PUMA20->CZ90 national file found.")
}
# 2) CZ20->* : group by CZ20
for (fp in cz20_files) {
df <- safe_read(fp)
bad <- check_sums(df, source_col = "CZ20", weight_col = "afactor", file_label = basename(fp))
outp <- file.path(qa_dir, paste0(tools::file_path_sans_ext(basename(fp)), "_QA_bad.csv"))
write_report(bad, outp)
if (nrow(bad)) summary_rows[[length(summary_rows)+1]] <- bad
}
# ---- combined summary ----
combined <- if (length(summary_rows)) dplyr::bind_rows(summary_rows) else tibble(
file = character(), source_col = character(), src = character(),
afactor_sum = numeric(), abs_err = numeric(), n_parts = integer(),
n_na = integer(), min_part = numeric(), max_part = numeric()
)
# 2) CZ20->* : group by CZ20
for (fp in cz20_files) {
df <- safe_read(fp)
bad <- check_sums(df, source_col = "CZ20", weight_col = "afactor", file_label = basename(fp))
outp <- file.path(qa_dir, paste0(tools::file_path_sans_ext(basename(fp)), "_QA_bad.csv"))
write_report(bad, outp)
if (nrow(bad)) summary_rows[[length(summary_rows)+1]] <- bad
}
# ---- combined summary ----
combined <- if (length(summary_rows)) dplyr::bind_rows(summary_rows) else tibble(
file = character(), source_col = character(), src = character(),
afactor_sum = numeric(), abs_err = numeric(), n_parts = integer(),
n_na = integer(), min_part = numeric(), max_part = numeric()
)
View(combined)
suppressPackageStartupMessages({
library(dplyr); library(readr); library(stringr)
})
# ---- paths ----
base_root <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk"
cross_dir <- file.path(base_root, "output", "crosswalks")
# ---- list of files you want to check ----
files <- c(
"cz20_to_cz90_popgrid100m_area_NATIONAL.csv",
"cz20_to_puma20_popgrid100m_area_NATIONAL.csv",
"cz20_to_puma2010_popgrid100m_area_NATIONAL.csv",
"cz20_to_cz2010_popgrid100m_area_NATIONAL.csv",
"cz20_to_cz2000_popgrid100m_area_NATIONAL.csv"
)
# ---- function to check one file ----
check_file <- function(fname, tol = 1e-6) {
fpath <- file.path(cross_dir, fname)
if (!file.exists(fpath)) {
message("File not found: ", fname)
return(NULL)
}
message("\nChecking: ", fname)
df <- readr::read_csv(fpath, show_col_types = FALSE)
bad <- df %>%
group_by(CZ20) %>%
summarise(sum_afactor = sum(afactor, na.rm = TRUE), .groups="drop") %>%
filter(abs(sum_afactor - 1) > tol)
if (nrow(bad) == 0) {
message("✅ All CZ20 afactor sums ≈ 1")
} else {
message("⚠️  Found ", nrow(bad), " CZ20(s) with bad afactor sums")
print(bad, n = 20)  # show up to 20
}
invisible(bad)
}
# ---- run check on all files ----
bad_list <- lapply(files, check_file)
# ---- list of files you want to check ----
files <- c(
"cz20_to_cz90_popgrid100m_area.csv",
"cz20_to_puma20_popgrid100m_area.csv",
"cz20_to_puma2010_popgrid100m_area.csv",
"cz20_to_cz2010_popgrid100m_area.csv",
"cz20_to_cz2000_popgrid100m_area.csv"
)
# ---- function to check one file ----
check_file <- function(fname, tol = 1e-6) {
fpath <- file.path(cross_dir, fname)
if (!file.exists(fpath)) {
message("File not found: ", fname)
return(NULL)
}
message("\nChecking: ", fname)
df <- readr::read_csv(fpath, show_col_types = FALSE)
bad <- df %>%
group_by(CZ20) %>%
summarise(sum_afactor = sum(afactor, na.rm = TRUE), .groups="drop") %>%
filter(abs(sum_afactor - 1) > tol)
if (nrow(bad) == 0) {
message("✅ All CZ20 afactor sums ≈ 1")
} else {
message("⚠️  Found ", nrow(bad), " CZ20(s) with bad afactor sums")
print(bad, n = 20)  # show up to 20
}
invisible(bad)
}
# ---- run check on all files ----
bad_list <- lapply(files, check_file)
# combine into one data.frame if you want to export
bad_all <- dplyr::bind_rows(
lapply(seq_along(bad_list), function(i) {
if (is.null(bad_list[[i]])) return(NULL)
bad_list[[i]] %>%
mutate(file = files[i])
})
)
View(bad_all)
# Adds PUMA code & GEOID (2010/2020) to the three crosswalk CSVs.
# Joins on: statefp (already in your CSVs) + the CSV's PUMA column.
# Output: same folder, with suffix "_enriched.csv".
library(sf)
library(dplyr)
library(readr)
library(stringr)
# ---- EDIT THESE PATHS ----
root <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk"
in_dir <- file.path(root, "input")
dir_2020  <- file.path(input_dir, "processed", "boundaries_2020")
dir_2020  <- file.path(in_dir, "processed", "boundaries_2020")
dir_2010  <- file.path(in_dir, "processed", "boundaries_2010")
puma20 <- read_std(file.path(dir_2020, "ipums_puma_2020.shp"))
# ---- add_puma_ids.R ---------------------------------------------------------
# Adds PUMA code & GEOID (2010/2020) to the three crosswalk CSVs.
library(sf)
library(dplyr)
library(readr)
library(stringr)
# ---- PATHS ----
# shapefiles
shp2010 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/input/processed/boundaries_2010/ipums_puma_2010_tl20.shp"
shp2020 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/input/processed/boundaries_2020/ipums_puma_2020.shp"
# csvs
csv_2010 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/cz20_to_puma2010_popgrid100m_area.csv"
csv_2020a <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/cz20_to_puma20_popgrid100m_area.csv"
csv_2020b <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/puma20_to_cz90_popgrid100m_area.csv"
# If your CSVs use different PUMA column names, list them (first found is used):
puma_cols_2010 <- c("puma2010","puma10","puma","puma_code")
puma_cols_2020 <- c("puma20","puma","puma_code")
# ---- helpers ----
pick_col <- function(df, candidates) {
hit <- candidates[candidates %in% names(df)]
if (length(hit) == 0) stop("None of these columns were found: ", paste(candidates, collapse=", "))
hit[1]
}
std_puma_lut <- function(gdf, year = c("2010","2020")) {
year <- match.arg(year)
gdf <- st_drop_geometry(gdf)
state_col <- intersect(c("STATEFP","STATEFP20","STATEFP10","STATEFP00"), names(gdf))[1]
if (is.na(state_col)) stop("STATEFP column not found in shapefile.")
statefp <- str_pad(as.character(gdf[[state_col]]), 2, "left", "0")
code_candidates <- c("PUMACE20","PUMACE10","PUMACE","PUMA5CE","PUMA","PUMACE_20","PUMACE_10")
code_col <- intersect(code_candidates, names(gdf))
code_col <- if (length(code_col)) code_col[1] else NA_character_
puma_code <- if (!is.na(code_col)) {
str_pad(str_extract(as.character(gdf[[code_col]]), "\\d+"), 5, "left", "0")
} else NA_character_
geoid_candidates <- c("GEOID20","GEOID10","GEOID")
geoid_col <- intersect(geoid_candidates, names(gdf))
geoid_col <- if (length(geoid_col)) geoid_col[1] else NA_character_
puma_geoid <- if (!is.na(geoid_col)) as.character(gdf[[geoid_col]]) else NA_character_
if (any(is.na(puma_geoid)) && !all(is.na(puma_code))) {
puma_geoid <- ifelse(is.na(puma_geoid),
paste0(statefp, puma_code),
puma_geoid)
}
if (any(is.na(puma_code)) && !all(is.na(puma_geoid))) {
puma_code <- ifelse(is.na(puma_code),
str_sub(puma_geoid, -5),
puma_code)
}
tibble(
statefp = statefp,
puma_code = str_pad(puma_code, 5, "left", "0"),
puma_geoid = str_pad(puma_geoid, 7, "left", "0")
) |>
distinct() |>
rename_with(~paste0(.x, "_", year), c(puma_code, puma_geoid))
}
enrich_file <- function(csv_path, lut, year, puma_candidates) {
message("Enriching: ", csv_path)
df <- read_csv(csv_path, show_col_types = FALSE) |>
mutate(statefp = str_pad(as.character(statefp), 2, "left", "0"))
pcol <- pick_col(df, puma_candidates)
df <- df |>
mutate(.puma_code_join = str_pad(str_extract(as.character(.data[[pcol]]), "\\d+"), 5, "left", "0"))
out <- df |>
left_join(lut, by = c("statefp" = paste0("statefp_", year),
".puma_code_join" = paste0("puma_code_", year))) |>
select(-.puma_code_join)
out_path <- sub("\\.csv$", "_enriched.csv", csv_path)
write_csv(out, out_path)
message("  -> wrote: ", out_path)
}
# ---- build lookups & run ----
lut10_raw <- st_read(shp2010, quiet = TRUE)
lut20_raw <- st_read(shp2020, quiet = TRUE)
# ---- add_puma_ids_by_GISJOIN.R -------------------------------------------
library(sf)
library(dplyr)
library(readr)
# 1) Paths ------------------------------------------------------------------
shp2010 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/input/processed/boundaries_2010/ipums_puma_2010_tl20.shp"
shp2020 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/input/processed/boundaries_2020/ipums_puma_2020.shp"
csv_2010 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/cz20_to_puma2010_popgrid100m_area.csv"
csv_2020a <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/cz20_to_puma20_popgrid100m_area.csv"
csv_2020b <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/puma20_to_cz90_popgrid100m_area.csv"
# 2) Read shapefiles and keep only GISJOIN, PUMA, GEOID ---------------------
p10 <- st_read(shp2010, quiet = TRUE) |>
st_drop_geometry() |>
select(GISJOIN, puma_code_2010 = PUMA, puma_geoid_2010 = GEOID)
p20 <- st_read(shp2020, quiet = TRUE) |>
st_drop_geometry() |>
select(GISJOIN, puma_code_2020 = PUMA, puma_geoid_2020 = GEOID)
# ---- add_puma_ids_by_GISJOIN.R -------------------------------------------
library(sf)
library(dplyr)
library(readr)
# 1) Paths ------------------------------------------------------------------
shp2010 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/input/processed/boundaries_2010/ipums_puma_2010_tl20.shp"
shp2020 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/input/processed/boundaries_2020/ipums_puma_2020.shp"
csv_2010 <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/cz20_to_puma2010_popgrid100m_area.csv"
csv_2020a <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/cz20_to_puma20_popgrid100m_area.csv"
csv_2020b <- "C:/Users/xiesi/ASU Dropbox/Xie Sijiao/Connor-Kemeny-Storper/Microdata_wealth_estimates/Crosswalk/CZ20_CZ90_xwalk/output/crosswalks/puma20_to_cz90_popgrid100m_area.csv"
# 2) Read shapefiles and keep only GISJOIN, PUMA, GEOID ---------------------
p10 <- st_read(shp2010, quiet = TRUE) |>
st_drop_geometry() |>
select(GISJOIN, puma_code_2010 = PUMA, puma_geoid_2010 = GEOID)
p20 <- st_read(shp2020, quiet = TRUE) |>
st_drop_geometry() |>
select(GISJOIN, puma_code_2020 = PUMA, puma_geoid_2020 = GEOID)
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
left_join(p10, by = "GISJOIN")
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE)
x2 <- read_csv(csv_2020a, show_col_types = FALSE) #|>
x3 <- read_csv(csv_2020b, show_col_types = FALSE)# |>
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
left_join(p10, by = c("PUMA2010"="GISJOIN"))
View(x1)
# 2) Read shapefiles and keep only GISJOIN, PUMA, GEOID ---------------------
p10 <- st_read(shp2010, quiet = TRUE) |>
st_drop_geometry() |>
select(GISJOIN, PUMA2010 = PUMA, PUMA10_GEOID = GEOID)
p20 <- st_read(shp2020, quiet = TRUE) |>
st_drop_geometry() |>
select(GISJOIN, PUMA2020 = PUMA, PUMA200_GEOID = GEOID)
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
rename(PUMA2010=PUMA2010_GISJOIN) |>
left_join(p10, by = c("PUMA2010"="GISJOIN"))
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
rename(PUMA2010_GISJOIN=PUMA2010) |>
left_join(p10, by = c("PUMA2010"="GISJOIN"))
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
rename(PUMA2010_GISJOIN=PUMA2010) |>
left_join(p10, by = c("PUMA2010_GISJOIN"="GISJOIN"))
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
rename(PUMA2010_GISJOIN=PUMA2010) |>
left_join(p10, by = c("PUMA2010_GISJOIN"="GISJOIN"))|>
select(CZ20, PUMA2010,everything())
View(x2)
View(x1)
# 3) Join to CSVs -----------------------------------------------------------
x1 <- read_csv(csv_2010, show_col_types = FALSE) |>
rename(PUMA2010_GISJOIN=PUMA2010) |>
left_join(p10, by = c("PUMA2010_GISJOIN"="GISJOIN"))|>
select(CZ20, PUMA2010,state_fips,afactor,everything())
View(x1)
write_csv(x1, sub("\\.csv$", "_enriched.csv", csv_2010))
write_csv(x1, sub("\\.csv$", "", csv_2010))
x2 <- read_csv(csv_2020a, show_col_types = FALSE) |>
rename(PUMA20_GISJOIN=PUMA20) |>
left_join(p20, by = c("PUMA20_GISJOIN"="GISJOIN"))|>
select(CZ20, PUMA2020,state_fips,afactor,everything())
View(x2)
write_csv(x2, sub("\\.csv$", "", csv_2020a))
x3 <- read_csv(csv_2020b, show_col_types = FALSE) #|>
x3 <- read_csv(csv_2020b, show_col_types = FALSE) |>
rename(PUMA20_GISJOIN=PUMA20) |>
left_join(p20, by = c("PUMA20_GISJOIN"="GISJOIN"))|>
select(PUMA2020,state_fips,CZ90, afactor,everything())
View(x3)
write_csv(x3, sub("\\.csv$", "", csv_2020b))
